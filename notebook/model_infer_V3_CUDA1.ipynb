{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f0ce2c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:22:08.359940Z",
     "start_time": "2022-02-12T00:22:08.345502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d2f506ce",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:24:41.223196Z",
     "start_time": "2022-02-12T00:24:41.198378Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from itertools import groupby\n",
    "import numpy as np\n",
    "import cv2\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "import matplotlib.pyplot as plt\n",
    "# import cupy as cp\n",
    "import ast\n",
    "from pathlib import Path\n",
    "import pickle5 as pickle\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append(\"../src/\")\n",
    "sys.path.append(\"../yolov5/\")\n",
    "\n",
    "import util\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "from mmdet.apis import init_detector, inference_detector\n",
    "import mmcv\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from sahi.utils.cv import read_image\n",
    "from sahi.utils.file import download_from_url\n",
    "from sahi.model import Yolov5DetectionModel\n",
    "from sahi.predict import get_prediction, get_sliced_prediction, predict\n",
    "from ensemble_boxes import weighted_boxes_fusion\n",
    "\n",
    "from IPython.display import Image\n",
    "from matplotlib import animation, rc\n",
    "rc('animation', html='jshtml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "169b6eea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:22:13.852348Z",
     "start_time": "2022-02-12T00:22:13.836459Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('1.10.1+cu102', '0.11.2+cu102')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__, torchvision.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e90818f1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:22:13.874050Z",
     "start_time": "2022-02-12T00:22:13.853167Z"
    }
   },
   "outputs": [],
   "source": [
    "CONF = 0.01\n",
    "IOU = 0.4\n",
    "AUGMENT = True # True "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "628ff3ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:22:13.894575Z",
     "start_time": "2022-02-12T00:22:13.875079Z"
    }
   },
   "outputs": [],
   "source": [
    "# add augmentation choice\n",
    "def perform_inference(self, image, image_size, augment=AUGMENT):\n",
    "    try:\n",
    "        import yolov5\n",
    "    except ImportError:\n",
    "        raise ImportError('Please run \"pip install -U yolov5\" ' \"to install YOLOv5 first for YOLOv5 inference.\")\n",
    "\n",
    "    # Confirm model is loaded\n",
    "    assert self.model is not None, \"Model is not loaded, load it by calling .load_model()\"\n",
    "\n",
    "    if image_size is not None:\n",
    "        warnings.warn(\"Set 'image_size' at DetectionModel init.\", DeprecationWarning)\n",
    "        prediction_result = self.model(image, size=image_size, augment=augment)\n",
    "    elif self.image_size is not None:\n",
    "        prediction_result = self.model(image, size=self.image_size, augment=augment)\n",
    "    else:\n",
    "        prediction_result = self.model(image, augment=augment)\n",
    "    self._original_predictions = prediction_result\n",
    "    return \n",
    "\n",
    "Yolov5DetectionModel.perform_inference = perform_inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b6005b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:22:13.911348Z",
     "start_time": "2022-02-12T00:22:13.895378Z"
    }
   },
   "outputs": [],
   "source": [
    "extra_param = {\n",
    "    \"0209_s6_B_LS02_newGT_img4096_valid\":{\n",
    "        'tools' : \"yolov5\",\n",
    "        'img_size' : 4096,\n",
    "        'ckpt_path': \"../input/best_valid_v2.pt\",\n",
    "    }\n",
    "}\n",
    "model_versions = ['0211_crcnn_1600']\n",
    "#model_versions = ['0209_m6_B_LS02_LGBT_newP']\n",
    "CUDA = \"cuda:1\"\n",
    "run_all=False\n",
    "cv_scheme = 'v2'#\"v2\"\n",
    "target_fold = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3684e82d",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbf678d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:22:13.955140Z",
     "start_time": "2022-02-12T00:22:13.912350Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23501, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT_DIR = Path(\"../../data/tensorflow-great-barrier-reef/\")\n",
    "df = pd.read_csv(INPUT_DIR / \"train.csv\")\n",
    "if cv_scheme == \"video_id\":\n",
    "    df['fold'] = df['video_id']\n",
    "elif cv_scheme == \"v2\":\n",
    "    folds = util.load_pickle(\"../input/fold_test_2.pkl\")\n",
    "    df[\"fold\"] = df[\"sequence\"].apply(lambda x: folds[x])\n",
    "    \n",
    "highFP_df = pd.read_csv('../input/df_highFPNoBB.csv')\n",
    "df = pd.merge(df, highFP_df[['video_id',\"video_frame\",\"highFBNoBB\"]], on=[\"video_id\",\"video_frame\"], how='left')\n",
    "df[\"highFBNoBB\"].fillna(False, inplace=True)\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c4eed09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:22:34.062019Z",
     "start_time": "2022-02-12T00:22:14.308812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb9be653b2da441494561665d3be1c10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d84d99ddf2a34d46b1e0a7c4cd5e3f9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ead5d637b82e427489af788fa759dfa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No BBox: 79.07% | With BBox: 20.93%\n"
     ]
    }
   ],
   "source": [
    "data_param = {'root_dir':INPUT_DIR,'label_dir':INPUT_DIR / \"labels\"}\n",
    "df = df.progress_apply(lambda x: util.get_path(x, data_param, infer=True), axis=1)\n",
    "df['annotations'] = df['annotations'].progress_apply(lambda x: ast.literal_eval(x))\n",
    "df[\"real_bbox\"] = df[\"annotations\"].apply(lambda annots: [list(annot.values()) for annot in annots])\n",
    "df['num_bbox'] = df['annotations'].progress_apply(lambda x: len(x))\n",
    "data = (df.num_bbox>0).value_counts(normalize=True)*100\n",
    "print(f\"No BBox: {data[0]:0.2f}% | With BBox: {data[1]:0.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c97008e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:22:34.099901Z",
     "start_time": "2022-02-12T00:22:34.063481Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    19173\n",
       "True      4328\n",
       "Name: train, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['train'] = False\n",
    "df.loc[df.query(f\"fold != {target_fold} and (num_bbox > 0 or highFBNoBB)\").index, 'train'] = True\n",
    "df['train'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e64374e5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:22:34.122934Z",
     "start_time": "2022-02-12T00:22:34.101280Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fold\n",
       "0    0.549803\n",
       "1    0.332484\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COTS per frame\n",
    "df.groupby(\"fold\").apply(lambda df: df[\"num_bbox\"].sum() / df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "851eb02c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:22:34.141662Z",
     "start_time": "2022-02-12T00:22:34.124012Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.799711\n",
       "1    0.200289\n",
       "Name: fold, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['fold'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5bd44aa",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef45f306",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:22:34.164015Z",
     "start_time": "2022-02-12T00:22:34.142395Z"
    }
   },
   "outputs": [],
   "source": [
    "meta_params = {}\n",
    "for model_version in model_versions:\n",
    "    model_folder = Path(f\"../output/{model_version}/\")\n",
    "    if not os.path.exists(model_folder):\n",
    "        meta_params[model_version] = extra_param[model_version]\n",
    "        continue\n",
    "    try:\n",
    "        params_path = model_folder / \"config\" / \"params.pkl\"\n",
    "        params = pickle.load(open(params_path, 'rb'))\n",
    "    except:\n",
    "        params_path = model_folder / \"config\" / \"params.yaml\"\n",
    "        params = util.load_yaml(params_path)\n",
    "        \n",
    "    for key, val in params.items():\n",
    "        if \"dir\" in key or \"path\" in key or \"file\" in key:\n",
    "            params[key] = Path(val)\n",
    "    meta_params[model_version] = params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fdc893e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:22:34.261018Z",
     "start_time": "2022-02-12T00:22:34.164765Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(1, len(meta_params),figsize=(5*len(meta_params), 5))\n",
    "if len(meta_params) == 1:\n",
    "    axes = [axes]\n",
    "target_score = {}\n",
    "for i,(model_version, params) in enumerate(meta_params.items()):\n",
    "    if params.get(\"tools\",\"mmdetection\") == \"mmdetection\" or model_version in extra_param.keys():\n",
    "        continue\n",
    "    logging_dir = params[\"ckpt_path\"].parent.parent\n",
    "    res_df = pd.read_csv(logging_dir / \"results.csv\")    \n",
    "    res_df[['       metrics/score','   metrics/precision', '      metrics/recall']].plot(ax=axes[i])\n",
    "    axes[i].legend()\n",
    "    axes[i].set_title(f\"{model_version} score\")    \n",
    "    target_score[model_version] = res_df['       metrics/score'].max()\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "855638bb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:22:34.278331Z",
     "start_time": "2022-02-12T00:22:34.261770Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83ccd37e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:25:33.837492Z",
     "start_time": "2022-02-12T00:25:31.289271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from local path: /home/vincent/Kaggle/Kaggle_TGBR/output/0211_crcnn_1600/best_bbox_mAP_epoch_8.pth\n"
     ]
    }
   ],
   "source": [
    "models = {}\n",
    "for model_version, params in meta_params.items():\n",
    "    if params['tools'] == \"yolov5\":\n",
    "        params[\"repo\"] = Path(\"../yolov5/\").resolve()\n",
    "    else:\n",
    "        ckp = glob(str(params['output_dir']) + \"/\" + \"best*\")[0]\n",
    "        params['ckpt_path'] = ckp\n",
    "    params[\"augment\"]  = AUGMENT\n",
    "    params[\"conf\"] = CONF\n",
    "    params[\"iou\"] = IOU\n",
    "\n",
    "    if \"slice\" in model_version:\n",
    "        model = Yolov5DetectionModel(\n",
    "            model_path=str(params['ckpt_path']),\n",
    "            confidence_threshold=0.01,\n",
    "            image_size = int(params['img_size'] * 1.3),\n",
    "            device=CUDA, # or 'cuda:0'\n",
    "        )    \n",
    "    else:\n",
    "        if params['tools']  == \"yolov5\":\n",
    "            model = util.load_model(params)\n",
    "            device = torch.device(CUDA)\n",
    "            model.to(device)\n",
    "        elif params['tools'] == \"mmdetection\":\n",
    "            test_pipeline = [\n",
    "                mmcv.utils.config.ConfigDict(type='LoadImageFromFile'),\n",
    "                mmcv.utils.config.ConfigDict(\n",
    "                    type='MultiScaleFlipAug',\n",
    "                    img_scale=[(params['img_size'], params['img_size'])] ,\n",
    "                    flip=[False],\n",
    "                    transforms=[\n",
    "                        mmcv.utils.config.ConfigDict(type='Resize', keep_ratio=False),\n",
    "                        mmcv.utils.config.ConfigDict(type='Pad', size_divisor=32),\n",
    "                        mmcv.utils.config.ConfigDict(type='RandomFlip', direction='horizontal'),\n",
    "                        mmcv.utils.config.ConfigDict(\n",
    "                            type='Normalize',\n",
    "                            mean=[123.675, 116.28, 103.53],\n",
    "                            std=[58.395, 57.12, 57.375],\n",
    "                            to_rgb=True),\n",
    "                        mmcv.utils.config.ConfigDict(type='ImageToTensor', keys=['img']),\n",
    "                        mmcv.utils.config.ConfigDict(type='Collect', keys=['img'])\n",
    "                    ])\n",
    "            ]\n",
    "\n",
    "            mm_config =  mmcv.Config.fromfile(str(params['cfg_dir'] / \"config.py\"))\n",
    "            mm_config['data']['test']['pipeline'] = test_pipeline\n",
    "            model = init_detector(mm_config, str(params['ckpt_path']), device='cuda:0')            \n",
    "    models[model_version] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb7e88a",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d0be6e0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:25:35.493682Z",
     "start_time": "2022-02-12T00:25:35.471362Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['video_id', 'sequence', 'video_frame', 'sequence_frame', 'image_id',\n",
       "       'annotations', 'fold', 'highFBNoBB', 'old_image_path', 'image_path',\n",
       "       'label_path', 'real_bbox', 'num_bbox', 'train', 'pred_0211_crcnn_1600'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[\"pred\"] = None\n",
    "for model_version in meta_params.keys():\n",
    "    df[\"pred_\" + model_version] = None\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "24cecc1d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:51:23.894641Z",
     "start_time": "2022-02-12T00:25:37.007247Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51ede31070c49fd97412ca5f6267826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/23501 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vincent/.local/lib/python3.8/site-packages/mmdet/datasets/utils.py:65: UserWarning: \"ImageToTensor\" pipeline is replaced by \"DefaultFormatBundle\" for batch inference. It is recommended to manually replace it in the test data pipeline in your config file.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "SH = 720\n",
    "SW = 720\n",
    "OHR = 0.2\n",
    "OWR = 0.2\n",
    "PMT = 0.4\n",
    "clahe = cv2.createCLAHE(clipLimit=5, tileGridSize=(10, 10))\n",
    "for idx in tqdm(range(len(df))):\n",
    "    row = df.loc[idx]\n",
    "    curr_fold = row['fold']\n",
    "    if (not run_all) and (curr_fold != target_fold):\n",
    "        continue\n",
    "    img_path = row[\"image_path\"]\n",
    "    img = cv2.imread(str(img_path))\n",
    "    for model_version, model in models.items():\n",
    "        SIZE = int(meta_params[model_version][\"img_size\"] * 1.3)\n",
    "        pred_col = \"pred_\" + model_version\n",
    "        if row[pred_col] is not None:\n",
    "            continue\n",
    "        USE_CLACHE = meta_params[model_version].get(\"use_clahe\", False)\n",
    "        img2 = img.copy()\n",
    "        if USE_CLACHE:\n",
    "            for i in range(3):\n",
    "                img2[:, :, i] = clahe.apply((img2[:, :, i]))     \n",
    "                \n",
    "        if meta_params[model_version][\"tools\"] == \"yolov5\":\n",
    "            img2 = img2[...,::-1]\n",
    "            if \"slice\" in model_version:\n",
    "                result_sliced = get_sliced_prediction(\n",
    "                    img2,\n",
    "                    model,\n",
    "                    slice_height = SH,\n",
    "                    slice_width = SW,\n",
    "                    overlap_height_ratio = OHR,\n",
    "                    overlap_width_ratio = OWR,\n",
    "                    postprocess_match_threshold = PMT,\n",
    "                    verbose = False)\n",
    "                object_prediction_list = result_sliced.object_prediction_list\n",
    "                confs = [obj_pred.score.value for obj_pred in object_prediction_list]\n",
    "                pred_bbox = np.array([obj_pred.bbox.to_coco_bbox() for obj_pred in object_prediction_list])\n",
    "            else:\n",
    "                pred_bbox, confs = util.predict(model, img2, size=SIZE, augment=AUGMENT, use_sahi=False)\n",
    "\n",
    "        elif meta_params[model_version][\"tools\"] == \"mmdetection\":\n",
    "            result = inference_detector(model, img2)\n",
    "            pred_bbox = result[0][:,:4]\n",
    "            pred_bbox[:,2:] = pred_bbox[:,2:] - pred_bbox[:,:2]\n",
    "            pred_confs = result[0][:,4]\n",
    "            confs = pred_confs.tolist()\n",
    "        df.at[idx, pred_col] = [ [conf] + pred_bbox[i].tolist() for i, conf in enumerate(confs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34baaa11",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:51:23.986048Z",
     "start_time": "2022-02-12T00:51:23.895391Z"
    }
   },
   "outputs": [],
   "source": [
    "#save it\n",
    "name = f\"pred_{model_versions[0]}.pkl\"\n",
    "if run_all:\n",
    "    name = \"whole_\" + name\n",
    "name = \"./oof_pred/\" + name\n",
    "util.save_pickle(df, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0af83403",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-12T00:51:24.003222Z",
     "start_time": "2022-02-12T00:51:23.986823Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./oof_pred/pred_0211_crcnn_1600.pkl'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "09a3f55c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-11T14:31:00.809546Z",
     "start_time": "2022-02-11T14:31:00.793942Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['video_id', 'sequence', 'video_frame', 'sequence_frame', 'image_id',\n",
       "       'annotations', 'fold', 'highFBNoBB', 'old_image_path', 'image_path',\n",
       "       'label_path', 'real_bbox', 'num_bbox', 'train',\n",
       "       'pred_0209_s6_B_LS02_newGT_img4096_valid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4241f629",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
